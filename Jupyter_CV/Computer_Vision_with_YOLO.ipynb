{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a70ddb0",
   "metadata": {},
   "source": [
    "# <center>  Μηχανική Όραση </center>\n",
    "\n",
    "<center> Εκπαίδευση μοντέλου μηχανικής όρασης με δυνατόττητες αναγνώρισης σε εικόνες, βίντεο και ζωντανή ροή. </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b406c60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Αυτή η συνεδρία δημιουργήθηκε από τον Γεώργιο Βαρβαρίγο. Επικοινωνία: giorgosvarvarigos03@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# Notebook Metadata\n",
    "author_name = \"Γεώργιο Βαρβαρίγο\"\n",
    "author_email = \"giorgosvarvarigos03@gmail.com\"\n",
    "\n",
    "print(f\"Αυτή η συνεδρία δημιουργήθηκε από τον {author_name}. Επικοινωνία: {author_email}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc25eca",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# <u> Εισαγωγή </u>\n",
    "Αυτή η συνεδρία έχει ως στόχο:\n",
    "- Να παρουσιάσει τα βήματα για τη δημιουργία ενός μοντέλου ανίχνευσης αντικειμένων.\n",
    "- Να δώσει πρακτική υλοποίηση με το YOLOv8 και το OpenCV.\n",
    "\n",
    "Αυτή η συνεδρία δεν έχει ως στόχο:\n",
    "- Να κάνει επιστημονική ανάλυση\n",
    "- Να εστιάσει σε επιστημονική μελέτη\n",
    "\n",
    "### Μεθοδολογία:\n",
    "1. Συλλογή δειγμάτων και δεδομένων.\n",
    "2. Επιλογή αλγορίθμου εκπαίδευσης\n",
    "3. Δόμηση των αρχείων.\n",
    "4. Εκπαίδευση και αξιολόγηση μοντέλου.\n",
    "5. Βελτιστοποίηση και μετατροπή.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129a764",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# <u> Θεωρητικό Υπόβαθρο </u>\n",
    "\n",
    "Το Νευρωνικό Δίκτυο\n",
    "\n",
    "- Στον πυρήνα πολλών συστημάτων τεχνητής νοημοσύνης βρίσκεται ένα νευρωνικό δίκτυο, μια υπολογιστική δομή εμπνευσμένη από τον ανθρώπινο εγκέφαλο. Είναι το θεμέλιο που επιτρέπει στις μηχανές να αναγνωρίζουν μοτίβα, να λαμβάνουν αποφάσεις και να μαθαίνουν.\n",
    "\n",
    "- Ένα νευρωνικό δίκτυο είναι ουσιαστικά μια συλλογή διασυνδεδεμένων νευρώνων, καθένας από τους οποίους εκτελεί μια απλή μαθηματική συνάρτηση.\n",
    "\n",
    "- Κάθε νευρώνας μεταβιβάζει το αποτέλεσμά του σε άλλους νευρώνες, δημιουργώντας ένα δίκτυο που επεξεργάζεται σταδιακά τα δεδομένα, βελτιώνοντάς τα ώστε να καταλήξουν στο επιθυμητό αποτέλεσμα.\n",
    "\n",
    "- Αντί να βασίζονται σε μια ενιαία, περίπλοκη εξίσωση, τα νευρωνικά δίκτυα χρησιμοποιούν πολλές μικρές συναρτήσεις για να κάνουν τη διαδικασία εκμάθησης επεκτάσιμη, δυναμική και ανθεκτική στα σφάλματα.\n",
    "\n",
    "- Το αποτέλεσμα ενός νευρωνικού δικτύου εξαρτάται από το σχεδιασμό και τον στόχο του. Μπορεί να ανιχνεύσει αντικείμενα σε μια εικόνα, να προσδιορίσει βασικά σημεία ενώς αντικειμένου ή να τμηματοποιήσει μια εικόνα σε διακριτές περιοχές.\n",
    "\n",
    "\n",
    "Η μηχανική όραση\n",
    "\n",
    "- Η μηχανική όραση (computer vision) είναι ένας τομέας της τεχνητής νοημοσύνης που εστιάζει στην ανάπτυξη αλγορίθμων και συστημάτων που επιτρέπουν στους υπολογιστές να κατανοούν, να ερμηνεύουν και να επεξεργάζονται οπτικά δεδομένα (όπως εικόνες και βίντεο) με τρόπο παρόμοιο με την ανθρώπινη όραση.\n",
    "- Αποτελεί τη «γέφυρα» ανάμεσα στη συλλογή οπτικών πληροφοριών μέσω καμερών ή αισθητήρων και την εξαγωγή χρήσιμων πληροφοριών από αυτές. Οι εφαρμογές της εκτείνονται από την καθημερινή ζωή μέχρι τη βιομηχανία, την ιατρική, την αυτοκίνηση και πολλές άλλες επιστήμες.\n",
    "\n",
    "\n",
    "### Πηγές:\n",
    "- https://el.wikipedia.org/wiki/%CE%9C%CE%B7%CF%87%CE%B1%CE%BD%CE%B9%CE%BA%CE%AE_%CE%BC%CE%AC%CE%B8%CE%B7%CF%83%CE%B7\n",
    "- https://www.cs.cmu.edu/~avrim/Talks/mlt.pdf\n",
    "- https://builtin.com/machine-learning/machine-learning-basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdb8c4",
   "metadata": {},
   "source": [
    "# <u> Υποκατηγορίες μηχανικής όρασης </u>\n",
    "\n",
    "Υπάρχουν υποκατηγορίες εφαρμογών της μηχανικής όρασης. Ανήκουν σε διαφορετικούς τομείς ανάλυσης εικόνας και βίντεο, με κάθε μία από αυτές να εστιάζει σε ξεχωριστές πτυχές της ερμηνείας των δεδομένων.\n",
    "\n",
    "### Παραδείγματα:\n",
    "\n",
    "- Object Detection (Ανίχνευση Αντικειμένων): Εντοπίζει και κατηγοριοποιεί αντικείμενα μέσα σε μια εικόνα ή βίντεο, επιστρέφοντας τα ορθογώνια πλαίσια (bounding boxes) που περιγράφουν τις θέσεις τους.\n",
    "- Segmentation (Τμηματοποίηση): Εστιάζει στην ακριβή οριοθέτηση αντικειμένων, επιστρέφοντας το περίγραμμα του αντικειμένου που εντοπίζει.\n",
    "- Object Relationships (Σχέσεις Μεταξύ Αντικειμένων): Αναγνωρίζει όχι μόνο τα αντικείμενα αλλά και τις σχέσεις τους. Για παράδειγμα, \"ο άνθρωπος κρατάει το ποτήρι\" ή \"ο σκύλος βρίσκεται κάτω από το τραπέζι\".\n",
    "- Keypoint Detection (Ανίχνευση Σημείων Κλειδιών): Προσδιορίζει με κουκίδες σημεία ενδιαφέροντος στο αντικείμενο, όπως οι αρθρώσεις του ανθρώπινου σώματος ή τα χαρακτηριστικά του προσώπου.\n",
    "\n",
    "Όλες οι υποκατηγορίες μηχανικής όρασης ακολουθούν παρόμοια λογική για την εκπαίδευση του τελικου μοντέλου. Ωστόσο υπάρχουν διαφορές στην επισήμανση των αντικειμένων στις εικόνες και στα frameworks που χρησιμοποιούνται ανά κλάδο. Στο υπάρχον υλικό επιλέχθηκε η κατηγορία ανίχνευσης αντικειμένων."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd611b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# <u> Συλλογή Δεδομένων </u>\n",
    "Στόχος: Η συλλογή υλικού για την εκπαίδευση του μοντέλου. Μεγαλύτερο υλικό εκπαίδευσης συνεπάγεται με καλύτερο αποτέλεσμα εντοπισμού στο τελικό μοντέλο.\n",
    "\n",
    "### Μέθοδοι:\n",
    "- <a href=\"https://storage.googleapis.com/openimages/web/index.html\" target=\"_blank\">Μαζική λήψη εικόνων από διάφορα Open Images Datasets.</a>\n",
    "- Εξαγωγή εικόνων από βίντεο.\n",
    "- Άμεση φωτογράφιση\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8b688",
   "metadata": {},
   "source": [
    "# <u> Επιλογή αλγορίθμου </u>\n",
    "\n",
    "Η επιλογή αλγορίθμου και framework για την ανάπτυξη ενός μοντέλου τεχνητής νοημοσύνης εξαρτάται από τις απαιτήσεις της εφαρμογής.\n",
    "\n",
    "### Εργαλεία:\n",
    "\n",
    "- <a href=\"https://pytorch.org/\" target=\"_blank\">Pytorch: Ιδιαίτερα δημοφιλές για ερευνητική εργασία και ευέλικτη ανάπτυξη λόγω της δυναμικής του φύσης</a>\n",
    "- <a href=\"https://www.tensorflow.org/\" target=\"_blank\">TensorFlow: Προτιμάται συχνά για παραγωγικά περιβάλλοντα λόγω της σταθερότητας και των εργαλείων που παρέχει</a>\n",
    "- <a href=\"https://docs.ultralytics.com/\" target=\"_blank\">YOLO: Eπικεντρώνεται στην ανίχνευση αντικειμένων, προσφέροντας υψηλή απόδοση και ευκολία χρήσης</a>\n",
    "\n",
    "Στο υπάρχον υλικό επιλέχθηκε ο αλγόριθμος YOLO του framework με το ίδιο όνομα"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdc0a78",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# <u> Επισήμανση εικόνων (image labeling) </u>\n",
    "\n",
    "Το image labeling είναι η διαδικασία κατά την οποία σε κάθε εικόνα προστίθενται πληροφορίες (ετικέτες) που περιγράφουν το περιεχόμενό της. Αυτή η διαδικασία είναι κρίσιμη για την εκπαίδευση αλγορίθμων μηχανικής όρασης, καθώς παρέχει τα δεδομένα που χρησιμοποιούνται για την εκμάθηση και τη βελτίωση των μοντέλων.\n",
    "\n",
    "### Ορολογία:\n",
    "- Annotation: Η πράξη προσθήκης πληροφοριών σε μια εικόνα, π.χ. ο σχεδιασμός ενός πλαισίου\n",
    "- Bounding Box: Ένα ορθογώνιο που σχεδιάζεται γύρω από ένα αντικείμενο για να προσδιορίσει τη θέση και το μέγεθός του μέσα στην εικόνα.\n",
    "- Class Label: Η περιγραφή του τύπου του αντικειμένου (π.χ. \"αυτοκίνητο\", \"γάτα\").\n",
    "- Background: Οτιδήποτε δεν έχει μπεί σε bounding box σε μια εικόνα θεωρείται παρασκήνιο. \n",
    "\n",
    "### Μεθοδολογία:\n",
    "1. Επισήμανση των Εικόνων: Χρησιμοποιούμε εφαρμογές επισήμανσης όπως <a href=\"https://pypi.org/project/labelImg/\" target=\"_blank\">LabelImg</a>, <a href=\"https://app.cvat.ai/\" target=\"_blank\">CVAT</a> και σχεδιάζουμε bounding boxes σε κάθε εικόνα γύρω από τα αντικείμενα που θέλουμε να ανιχνεύσει το μοντέλο. Κάθε box αντιστοιχίζεται σε μια κατηγορία (class).\n",
    "2. Δημιουργία Αρχείων Label: Για κάθε εικόνα που επισημαίνεται, παράγεται ένα label αρχείο (συνήθως .txt) με το ίδιο όνομα όπως η εικόνα. Στο YOLO framework κάθε label αντιπροσωπεύει ένα αντικείμενο και είναι της μορφής: `class x_center y_center width height` όπου class_id ο αριθμός της κατηγορίας του αντικειμένου (ξεκινά από το 0), x_center & y_center οι συντεταγμένες του κέντρου του bounding box, width & height το πλάτος και ύψος του bounding box.\n",
    "3. Δημιουργία Αρχείου Classes: Ένα αρχείο classes.txt που περιέχει όλα τα classes. Κάθε γραμμή περιέχει το όνομα ενός class. Η σειρά των classes καθορίζεται από τα αντίστοιχα class_id (ξεκινώντας από το 0) π.χ. `cat\\n dog\\n fish`\n",
    "\n",
    "\n",
    "![SegmentLocal](shiba_annotation.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b11362",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## <u> Οργάνωση Δεδομένων <u/>\n",
    "\n",
    "Τα δεδομένα αυστηρά πρέπει να οργανωθούν σε φακέλους. Δημιουργούμαι τρία sets με εικόνες που θα χρησιμοποιηθούν για εκμάθηση του μοντέλου (train), εικόνες που θα χρησιμοποιηθούν από το μοντέλο για επικύρωση των ικανοτήτων του (validation) και εικόνες για περαιτέρω δοκιμές (test). Προτείνεται ο συνολικός αριθμός των εικόνων να μοιράζεται 70% για εκμάθηση, 15% για επικύρωση και 15% για δοκιμές. Στο YOLO framework η δομή είναι η ακόλουθη:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d13607",
   "metadata": {},
   "outputs": [],
   "source": [
    "    config.yaml\n",
    "    dataset/\n",
    "├── images/\n",
    "│   ├── train/\n",
    "│   ├── val/\n",
    "│   ├── test/\n",
    "├── labels/\n",
    "│   ├── train/\n",
    "│   ├── val/\n",
    "│   ├── test/\n",
    "├── classes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be69a12",
   "metadata": {},
   "source": [
    "### Όπου:\n",
    "- `images/train`: Εικόνες για εκπαίδευση.\n",
    "- `labels/train`: Τα αντιστοιχά labels.\n",
    "- `images/val`: Εικόνες για επικύρωση. \n",
    "- `labels/val`: Τα αντιστοιχά labels.\n",
    "- `images/test`: Εικόνες για δοκιμές. \n",
    "- `labels/test`: Τα αντιστοιχά labels.\n",
    "- `config.yaml`: Απαραίτητο όταν η εκπαίδευση γίνεται με χρήση collab notebook (αναλύεται παρακάτω)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8fd11d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## <u> Εκπαίδευση Μοντέλου <u/>\n",
    "\n",
    "Σε αυτό το στάδιο, το νευρωνικό δίκτυο \"μαθαίνει\" να αναγνωρίζει μοτίβα και να παίρνει αποφάσεις με βάση τα δεδομένα που του παρέχουμε. Κατά την εκπαίδευση, το δίκτυο επεξεργάζεται εκατοντάδες ή και χιλιάδες εικόνες, προσαρμόζει τις παραμέτρους του και σταδιακά βελτιώνει την ακρίβειά του μέσω επαναλήψεων. Η εκπαίδευση έχει μεγάλες υπολογιστικές απαιτήσεις και γίνεται είτε σε τοπικό υπολογιστή είτε ~~όταν ο υπολογιστής είναι τοστιέρα~~ σε collab notebook, όπου παρέχεται δωρεάν πρόσβαση σε υπολογιστικούς πόρους.\n",
    "\n",
    "### Παράμετροι:\n",
    "- Epochs: Σε κάθε epoch, το μοντέλο βλέπει όλα τα δεδομένα εκπαίδευσης μία φορά. Με περισσότερα epochs, το μοντέλο εκπαιδεύεται καλύτερα, αλλά υπάρχει κίνδυνος υπερπροσαρμογής (overfitting), όπου αποδίδει καλά μόνο στο σύνολο εκπαίδευσης και όχι στα νέα δεδομένα.\n",
    "- Batch Size: Το πλήθος των δειγμάτων που επεξεργάζεται το μοντέλο σε κάθε βήμα κατά τη διάρκεια της εκπαίδευσης, π.χ. Αν έχει 1.000 εικόνες και batch size 10, το μοντέλο θα χρειαστεί 100 βήματα για να ολοκληρώσει 1 epoch. Μεγάλο batch size υπόσχεται μεγαλύτερη ταχύτητα εκπαίδευσεις σε αντάλλαγμα με αυξημένες απαιτήσεις από την GPU\n",
    "- Ιmage Σize: Το μέγεθος (σε pixels) στο οποίο μετατρέπονται οι εικόνες εισόδου πριν χρησιμοποιηθούν στην εκπαίδευση. Μεγάλες εικόνες υπόσχονται αυξημένη ακρίβεια με μεγαλύτερες απαιτήσεις από την GPU και μειωμένη ταχύτητα εκπαίδευσης.\n",
    "- <a href=\"https://docs.ultralytics.com/usage/cfg/\" target=\"_blank\">Και άλλα</a>\n",
    "    \n",
    "### Τύποι μοντέλων:\n",
    "Κατά την εκπαίδευση του μοντέλου επιλέγεται ένας τύπος ο οποίος καθορίζει το μέγεθος και την αρχιτεκτονική του τελικού μοντέλου. Διακρίνονται σε nano (yolov8n.pt), small (yolov8s.pt), medium (yolov8m.pt), large (yolov8l.pt), extra-large (yolov8x.pt). Όσο μικρότερο το μέγεθος τόσο μικρότερη ακρίβεια αλλά μεγαλύτερη ταχύτητα απόκρισης και μικρότερες υπολογιστικές απαιτήσεις.\n",
    "\n",
    "### Παράδειγμα\n",
    "Το ακόλουθο παράδειγμα παρουσιάζει την διαδικασία εκπάιδευσης ενός μοντέλου YOLOv8 σε γλώσσα Python μέσω collab notebook. Τα δεδομένα τοποθετούνται στο google drive. Ένα config.yaml αρχείο είναι απαραίτητο και περιέχει βασικά στοιχεία και πληροφορίες για την εκπαίδευση όπως μονοπάτια φακέλων και ονόματα κλάσεων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b96158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Παράδειγμα config.yaml\n",
    "path: '/content/gdrive/My Drive/strawberries/data' # dataset root dir\n",
    "train: images/train  # train images (relative to 'path')\n",
    "val: images/val  # val images (relative to 'path')\n",
    "\n",
    "names:\n",
    "  0: strawberry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe6b44",
   "metadata": {},
   "source": [
    "Ο κώδικας μοιρασμένος σε κομμάτια"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef1b206",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Mount Google Drive ###\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab5416f5",
   "metadata": {},
   "source": [
    "Mounted at /content/gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9207e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. Define root directory ###\n",
    "\n",
    "ROOT_DIR = '/content/gdrive/My Drive/strawberries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0889624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Install Ultralytics ###\n",
    "\n",
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2976940",
   "metadata": {},
   "outputs": [],
   "source": [
    "Collecting ultralytics\n",
    "  Downloading ultralytics-8.2.78-py3-none-any.whl.metadata (41 kB)\n",
    "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.3/41.3 kB 1.9 MB/s eta 0:00:00\n",
    "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
    "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
    "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
    "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
    "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
    "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
    "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
    "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.1+cu121)\n",
    "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.1+cu121)\n",
    "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
    "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
    "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
    "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
    "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
    "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
    "  Downloading ultralytics_thop-2.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
    "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
    "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
    "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
    "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
    "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
    "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
    "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
    "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
    "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
    "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
    "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
    "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
    "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n",
    "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
    "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
    "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
    "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
    "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
    "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
    "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
    "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
    "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
    "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
    "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
    "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
    "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n",
    "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
    "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
    "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
    "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
    "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
    "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
    "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
    "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
    "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
    "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
    "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
    "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n",
    "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
    "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
    "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
    "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.1)\n",
    "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
    "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
    "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
    "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
    "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
    "Downloading ultralytics-8.2.78-py3-none-any.whl (869 kB)\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 869.0/869.0 kB 20.0 MB/s eta 0:00:00\n",
    "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
    "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
    "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
    "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
    "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
    "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
    "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
    "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
    "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
    "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
    "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
    "Downloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\n",
    "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
    "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
    "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 ultralytics-8.2.78 ultralytics-thop-2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e787c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Train model ###\n",
    "\n",
    "import os\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n.pt\")  # load pre trained model\n",
    "\n",
    "# Use the model\n",
    "results = model.train(data=os.path.join(ROOT_DIR, \"google_colab_config.yaml\"), epochs=20)  # train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2a56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'yolov8n.pt'...\n",
    "\n",
    "100%|██████████| 6.25M/6.25M [00:00<00:00, 90.5MB/s]\n",
    "\n",
    "Ultralytics YOLOv8.2.78 🚀 Python-3.10.12 torch-2.3.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
    "engine/trainer: task=detect, mode=train, model=yolov8n.pt, data=/content/gdrive/My Drive/strawberries/google_colab_config.yaml, epochs=20, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
    "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
    "\n",
    "100%|██████████| 755k/755k [00:00<00:00, 21.0MB/s]\n",
    "\n",
    "Overriding model.yaml nc=80 with nc=1\n",
    "\n",
    "                   from  n    params  module                                       arguments                     \n",
    "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
    "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
    "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
    "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
    "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
    "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
    "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
    "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
    "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
    "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
    " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
    " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
    " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
    " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
    " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
    " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
    " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
    " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
    " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
    " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
    " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
    " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
    " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
    "Model summary: 225 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
    "\n",
    "Transferred 319/355 items from pretrained weights\n",
    "TensorBoard: Start with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
    "Freezing layer 'model.22.dfl.conv.weight'\n",
    "\n",
    "train: Scanning /content/gdrive/My Drive/strawberries/data/labels/train... 164 images, 0 backgrounds, 0 corrupt: 100%|██████████| 164/164 [02:02<00:00,  1.33it/s]\n",
    "\n",
    "train: New cache created: /content/gdrive/My Drive/strawberries/data/labels/train.cache\n",
    "albumentations: Blur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
    "\n",
    "val: Scanning /content/gdrive/My Drive/strawberries/data/labels/val... 41 images, 0 backgrounds, 0 corrupt: 100%|██████████| 41/41 [01:07<00:00,  1.65s/it]\n",
    "\n",
    "val: New cache created: /content/gdrive/My Drive/strawberries/data/labels/val.cache\n",
    "\n",
    "\n",
    "Plotting labels to runs/detect/train/labels.jpg... \n",
    "optimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
    "optimizer: AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
    "TensorBoard: model graph visualization added ✅\n",
    "Image sizes 640 train, 640 val\n",
    "Using 0 dataloader workers\n",
    "Logging results to runs/detect/train\n",
    "Starting training for 20 epochs...\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "       1/20         0G     0.8133      2.677      1.032         18        640: 100%|██████████| 11/11 [04:12<00:00, 22.94s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:43<00:00, 21.65s/it]\n",
    "\n",
    "                   all         41        156      0.232      0.699      0.578      0.413\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "       2/20         0G     0.8666      1.074      1.004         23        640: 100%|██████████| 11/11 [02:37<00:00, 14.30s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:17<00:00,  8.78s/it]\n",
    "\n",
    "                   all         41        156          1      0.246      0.908      0.688\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "       3/20         0G     0.7938     0.9535     0.9543         25        640: 100%|██████████| 11/11 [02:33<00:00, 13.96s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:18<00:00,  9.16s/it]\n",
    "\n",
    "                   all         41        156          1      0.399      0.924      0.729\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "       4/20         0G     0.7688     0.8282      0.958         19        640: 100%|██████████| 11/11 [02:34<00:00, 14.04s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:20<00:00, 10.45s/it]\n",
    "\n",
    "                   all         41        156          1      0.448      0.914      0.717\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "       5/20         0G     0.7167     0.7598     0.9269         35        640: 100%|██████████| 11/11 [02:31<00:00, 13.73s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:18<00:00,  9.09s/it]\n",
    "\n",
    "                   all         41        156      0.992      0.838      0.972      0.746\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "       6/20         0G     0.6948     0.7604     0.9209         13        640: 100%|██████████| 11/11 [02:29<00:00, 13.56s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:18<00:00,  9.17s/it]\n",
    "\n",
    "                   all         41        156      0.993      0.854      0.958       0.76\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "       7/20         0G     0.6815     0.7026      0.908         27        640: 100%|██████████| 11/11 [02:36<00:00, 14.22s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:18<00:00,  9.22s/it]\n",
    "\n",
    "                   all         41        156      0.994       0.98      0.993       0.79\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "       8/20         0G     0.6732     0.6905     0.9333         28        640: 100%|██████████| 11/11 [02:33<00:00, 13.96s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:17<00:00,  8.78s/it]\n",
    "\n",
    "                   all         41        156      0.989      0.872      0.944      0.754\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "       9/20         0G     0.6706      0.695     0.9115         16        640: 100%|██████████| 11/11 [02:33<00:00, 13.92s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:19<00:00,  9.60s/it]\n",
    "\n",
    "                   all         41        156      0.976      0.777      0.847      0.691\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "      10/20         0G     0.6429      0.661     0.9185         15        640: 100%|██████████| 11/11 [02:32<00:00, 13.87s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:17<00:00,  8.65s/it]\n",
    "\n",
    "                   all         41        156      0.977      0.942      0.981      0.791\n",
    "\n",
    "\n",
    "Closing dataloader mosaic\n",
    "albumentations: Blur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "      11/20         0G     0.5962     0.6955     0.8875         13        640: 100%|██████████| 11/11 [02:34<00:00, 14.05s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:18<00:00,  9.04s/it]\n",
    "\n",
    "                   all         41        156      0.973      0.909      0.958      0.798\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "      12/20         0G     0.6088      0.695      0.872         12        640: 100%|██████████| 11/11 [02:27<00:00, 13.41s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:17<00:00,  8.68s/it]\n",
    "\n",
    "                   all         41        156       0.98      0.951      0.987      0.809\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "      13/20         0G     0.5543      0.626     0.8639         14        640: 100%|██████████| 11/11 [02:29<00:00, 13.59s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:20<00:00, 10.39s/it]\n",
    "\n",
    "                   all         41        156      0.936      0.943      0.981       0.82\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "      14/20         0G     0.5682     0.6342     0.8759         15        640: 100%|██████████| 11/11 [02:32<00:00, 13.90s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:19<00:00,  9.63s/it]\n",
    "\n",
    "                   all         41        156      0.967      0.974       0.99      0.811\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "      15/20         0G      0.581     0.6175     0.8682         12        640: 100%|██████████| 11/11 [02:29<00:00, 13.61s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:18<00:00,  9.08s/it]\n",
    "\n",
    "                   all         41        156      0.975      0.936      0.989      0.841\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "      16/20         0G     0.5229     0.5556     0.8538         17        640: 100%|██████████| 11/11 [02:29<00:00, 13.64s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:17<00:00,  8.79s/it]\n",
    "\n",
    "                   all         41        156      0.971      0.987      0.994      0.827\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "      17/20         0G     0.5309     0.5553     0.8476         12        640: 100%|██████████| 11/11 [02:29<00:00, 13.57s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:19<00:00,  9.89s/it]\n",
    "\n",
    "                   all         41        156      0.975      0.992      0.994      0.828\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "      18/20         0G     0.5107     0.5448     0.8505         13        640: 100%|██████████| 11/11 [02:32<00:00, 13.91s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:17<00:00,  8.64s/it]\n",
    "\n",
    "                   all         41        156      0.975      0.994      0.994      0.835\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "      19/20         0G       0.52     0.5388     0.8664         11        640: 100%|██████████| 11/11 [02:31<00:00, 13.81s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:18<00:00,  9.02s/it]\n",
    "\n",
    "                   all         41        156      0.983      0.981      0.994      0.837\n",
    "\n",
    "\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "\n",
    "      20/20         0G     0.4968     0.5142     0.8437         21        640: 100%|██████████| 11/11 [02:27<00:00, 13.42s/it]\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:18<00:00,  9.03s/it]\n",
    "\n",
    "                   all         41        156      0.988      0.994      0.994      0.846\n",
    "\n",
    "\n",
    "\n",
    "20 epochs completed in 0.988 hours.\n",
    "Optimizer stripped from runs/detect/train/weights/last.pt, 6.2MB\n",
    "Optimizer stripped from runs/detect/train/weights/best.pt, 6.2MB\n",
    "\n",
    "Validating runs/detect/train/weights/best.pt...\n",
    "Ultralytics YOLOv8.2.78 🚀 Python-3.10.12 torch-2.3.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
    "Model summary (fused): 168 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
    "\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:20<00:00, 10.04s/it]\n",
    "\n",
    "                   all         41        156      0.988      0.994      0.994      0.846\n",
    "Speed: 2.5ms preprocess, 252.4ms inference, 0.0ms loss, 11.6ms postprocess per image\n",
    "Results saved to runs/detect/train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Copy results ###\n",
    "\n",
    "!scp -r /content/runs '/content/gdrive/My Drive/strawberries'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbd9e9d",
   "metadata": {},
   "source": [
    "## <u> Αξιολόγηση Μοντέλου </u>\n",
    "\n",
    "Η ποιότητα ενός μοντέλου μηχανικής μάθησης μπορεί να εκτιμηθεί μέσα από διάφορες μετρικές και διαγράμματα που παράγονται κατά τη διαδικασία εκπαίδευσης και αξιολόγησης. Με την ανάλυση αυτών των στοιχείων, μπορούμε να προβλέψουμε την απόδοση του μοντέλου σε πραγματικές συνθήκες. Παρατίθενται τα αποτελέσματα από δύο διαφορετικής ποιότητας μοντέλα εκπαιδευμένα στο εντοπισμό φραουλών.\n",
    "\n",
    "### Confusion Matrix:\n",
    "-  Ο πίνακας σύγχισης (confusion matrix) παρουσιάζει τη σχέση μεταξύ των προβλέψεων μιας κλάσης και του παρασκηνίου σε σχέση με τα πραγματικά δεδομένα που δώθηκαν για εκπαίδευση.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center;\">\n",
    "    <div style=\"text-align: center; margin: 10px;\">\n",
    "        <img src=\"confusion_matrix.png\" style=\"width: 500%;\">\n",
    "        <p>Καλό μοντέλο</p>\n",
    "        <p>Ο μεγαλύτερος αριθμός δειγμάτων εκτιμήθηκε σωστά (Τrue Positive)</p>\n",
    "        <p>Ένας αμελητέος αριθμός προβλέψεων εντόπισε δείγματα που δεν υπάρχουν (False Positive)</p>\n",
    "    </div>\n",
    "    <div style=\"text-align: center; margin: 10px;\">\n",
    "        <img src=\"bad_confusion_matrix.png\" style=\"width: 500%;\">\n",
    "        <p>Κακό μοντέλο</p>\n",
    "        <p>Κανένα δείγμα δεν εκτιμήθηκε σωστά</p>\n",
    "        <p>Λόγο του μικρού αριθμού δεδομένων που δώθηκαν για εκπαίδευση όλα τα labels εκτιμήθηκαν ως παρασκήνιο (False Negative)</p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Αποτελέσματα εικόνων επικύρωσης (validation)\n",
    "- Οι εικόνες που χρησιμοποιήθηκαν για validation χρσηιμοποιούνται για να εκτιμηθεί η ικανότητα του τελικού μοντέλου. Με το πέραν της εκπαίδευσης επιστρέφονται σημειωμένες με όσες κλάσεις εντόπισε το μοντέλο.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center;\">\n",
    "    <div style=\"text-align: center; margin: 10px;\">\n",
    "        <img src=\"val_batch1_pred.jpg\" style=\"width: 500%;\">\n",
    "        <p>Καλό μοντέλο</p>\n",
    "        <p>Εντοπίστηκαν όλα τα αντικείμενα ενδιαφέροντος</p>\n",
    "    </div>\n",
    "    <div style=\"text-align: center; margin: 10px;\">\n",
    "        <img src=\"bad_val_batch0_pred.jpg\" style=\"width: 500%;\">\n",
    "        <p>Κακό μοντέλο</p>\n",
    "        <p>Δεν εντοπίστηκε τίποτα</p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ebf84",
   "metadata": {},
   "source": [
    "## <u> Δοκιμές Μοντέλου </u>\n",
    "\n",
    "Όταν η διαδικασία εκπαίδευσης ολοκληρωθεί μαζί με τα διάφορα διαγράμματα παράγεται και ένα αρχείο με επέκταση .pt το οποίο είναι το τελικό μοντέλο έτοιμο προς χρήση. Απαραίτητες βιβλιοθήκες είναι η <a href=\"https://pypi.org/project/ultralytics/\" target=\"_blank\">ultralyitcs</a> για την αξιοποίηση του τελικού μοντέλου και η <a href=\"https://pypi.org/project/opencv-python/\" target=\"_blank\">opencv-python</a>, μια διάσημη βιβλιοθήκη με γενικού περιεχομένου εργαλεία μηχανικής όρασης για οποιουδήποτε τύπου μοντέλο. Παρακάτω ακολουθεί ένας κώδικας σε python παρμένος από <a href=\"https://dipankarmedh1.medium.com/real-time-object-detection-with-yolo-and-webcam-enhancing-your-computer-vision-skills-861b97c78993\" target=\"_blank\">εδώ</a> που χρησιμοποιεί ένα μοντέλο για ζωντανής μετάδοσης αναγνώριση αντικειμένων με την χρήση της κάμερας του υπολογιστή."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10eca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import math \n",
    "# start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640)\n",
    "cap.set(4, 480)\n",
    "\n",
    "# model\n",
    "model = YOLO(\"model.pt\")\n",
    "\n",
    "# object classes\n",
    "classNames = [\"truck\", \"train\", \"car\", \"traffic light\", \"stop sign\"]\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    results = model(img, stream=True)\n",
    "\n",
    "    # coordinates\n",
    "    for r in results:\n",
    "        boxes = r.boxes\n",
    "\n",
    "        for box in boxes:\n",
    "            # bounding box\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2) # convert to int values\n",
    "\n",
    "            # put box in cam\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "\n",
    "            # confidence\n",
    "            confidence = math.ceil((box.conf[0]*100))/100\n",
    "            print(\"Confidence --->\",confidence)\n",
    "\n",
    "            # class name\n",
    "            cls = int(box.cls[0])\n",
    "            print(\"Class name -->\", classNames[cls])\n",
    "\n",
    "            # object details\n",
    "            org = [x1, y1]\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            fontScale = 1\n",
    "            color = (255, 0, 0)\n",
    "            thickness = 2\n",
    "\n",
    "            cv2.putText(img, classNames[cls], org, font, fontScale, color, thickness)\n",
    "\n",
    "    cv2.imshow('Webcam', img)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a868863a",
   "metadata": {},
   "source": [
    "![SegmentLocal](cv_yolo.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c70739",
   "metadata": {},
   "source": [
    "## <u> Μετατροπή Μοντέλου (ONNX) </u>\n",
    "\n",
    "Το <a href=\"https://onnx.ai/\" target=\"_blank\">ONNX (Open Neural Network Exchange)</a> είναι ένα ανοιχτό πρότυπο στόχος του οποίου να διευκολύνει τη διαλειτουργικότητα μεταξύ διαφορετικών frameworks μηχανικής όρασης, μετατρέποντας ένα μοντέλο σε μια μορφή κατανοητή από πολλές διαφορετικές συσκευές και προγράμματα. Η μετατροπή ενός μοντέλου σε μορφή ONNX παρέχει πλήθος πλεονεκτημάτων σε συσκευές περιορισμένης υπολογιστικής δύναμης.\n",
    "\n",
    "### Πλεονεκτήματα ONNX:\n",
    "- Διαλειτουργικότητα (Interoperability): Τα μοντέλα που εκπαιδεύονται σε διαφορετικά frameworks, όπως PyTorch ή TensorFlow μπορούν να μετατραπούν σε ONNX και να εκτελεστούν ανεξάρτητα από το αρχικό framework.\n",
    "- Απόδοση (Performance):Το ONNX Runtime, απαραίτητο να εγκατασταθεί για να λειτουργήσει ένα ONNX μοντέλο, αξιοποιεί ειδικές τεχνολογίες για να παρέχει γρήγορη εκτέλεση μοντέλων σε CPUs, GPUs και edge συσκευές.\n",
    "- Ελαφρότητα και Ευελιξία: Είναι ιδανικό για low-end συσκευές και edge εφαρμογές, καθώς έχει χαμηλές απαιτήσεις σε υπολογιστική ισχύ και μνήμη.\n",
    "- Υποστηρίζει Hardware Acceleration\n",
    "- Είναι ευρέως υποστηριζόμενο από πολλά μεγάλα frameworks μηχανικής μάθησης\n",
    "\n",
    "\n",
    "Παρακάτω παρατίθεται ένας κώδικας μετατροπής για collab notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5695e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install the ultralytics package\n",
    "!pip install ultralytics\n",
    "\n",
    "# Step 2: Import the required libraries\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Step 3: Upload your custom .pt file\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Step 4: Load your custom YOLO model\n",
    "model_name = 'best.pt'  # Replace with your model file name\n",
    "model = YOLO(model_name)\n",
    "\n",
    "# Step 5: Export the model to ONNX\n",
    "input_width = 640  # Adjust as needed\n",
    "input_height = 480  # Adjust as needed\n",
    "optimize_cpu = True  # Set to True to optimize for CPU\n",
    "\n",
    "model.export(format=\"onnx\", imgsz=[input_height, input_width], optimize=optimize_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2506c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.2.61)\n",
    "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.25.2)\n",
    "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
    "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
    "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
    "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
    "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
    "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
    "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.1+cu121)\n",
    "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.1+cu121)\n",
    "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n",
    "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
    "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
    "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n",
    "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
    "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.0)\n",
    "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
    "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
    "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
    "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
    "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
    "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
    "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
    "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
    "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
    "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
    "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
    "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
    "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n",
    "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
    "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
    "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.0)\n",
    "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
    "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
    "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
    "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
    "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
    "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
    "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (8.9.2.26)\n",
    "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n",
    "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n",
    "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n",
    "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n",
    "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n",
    "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.20.5)\n",
    "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
    "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.1)\n",
    "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.5.82)\n",
    "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
    "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
    "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
    "\n",
    "Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable.\n",
    "\n",
    "Ultralytics YOLOv8.2.61 🚀 Python-3.10.12 torch-2.3.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
    "Model summary (fused): 168 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
    "\n",
    "PyTorch: starting from 'best.pt' with input shape (1, 3, 480, 640) BCHW and output shape(s) (1, 5, 6300) (6.0 MB)\n",
    "requirements: Ultralytics requirement ['onnx>=1.12.0'] not found, attempting AutoUpdate...\n",
    "Collecting onnx>=1.12.0\n",
    "  Downloading onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
    "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.9/15.9 MB 242.6 MB/s eta 0:00:00\n",
    "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (1.25.2)\n",
    "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (3.20.3)\n",
    "Installing collected packages: onnx\n",
    "Successfully installed onnx-1.16.1\n",
    "\n",
    "requirements: AutoUpdate success ✅ 10.9s, installed 1 package: ['onnx>=1.12.0']\n",
    "requirements: ⚠️ Restart runtime or rerun command for updates to take effect\n",
    "\n",
    "\n",
    "ONNX: starting export with onnx 1.16.1 opset 17...\n",
    "ONNX: export success ✅ 12.2s, saved as 'best.onnx' (11.6 MB)\n",
    "\n",
    "Export complete (14.6s)\n",
    "Results saved to /content\n",
    "Predict:         yolo predict task=detect model=best.onnx imgsz=480,640  \n",
    "Validate:        yolo val task=detect model=best.onnx imgsz=480,640 data=/content/gdrive/My Drive/ObjDetStraw/google_colab_config.yaml  WARNING ⚠️ non-PyTorch val requires square images, 'imgsz=[480, 640]' will not work. Use export 'imgsz=640' if val is required.\n",
    "Visualize:       https://netron.app\n",
    "\n",
    "'best.onnx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0717cb75",
   "metadata": {},
   "source": [
    "<img src=\"strawberries_falling.gif\" width=\"1000\" height=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a7fdb",
   "metadata": {},
   "source": [
    "## <u> Κβαντισμός (Quantization) </u>\n",
    "\n",
    "- O κβαντισμός μας βοηθά να συρρικνώσουμε τα πολύπλοκα μοντέλα, ώστε να μπορούν να ταιριάζουν και να λειτουργούν ομαλά σε μικρότερες συσκευές περιορισμένης υπολογιστικής δύναμης. Πολλά framework παρέχουν εργαλεία κβαντισμού για μετά την εκπαίδευση ή και κατά την διάρκεια της εκπαίδευσης ωστόσο η μετατροπή ενός μοντέλου σε μορφή ONNX ως ενδιάμεσο βήμα κάνει την διαδικάσια ευκολότερη και πιο αποδοτική.\n",
    "- Η φύση του κβαντισμού είναι η ελάττωση της ακρίβειας των αριθμητικών πράξεων σε ένα νευρωνικό δίκτυο και κατά συνέπεια επιρεάζει την ακρίβεια του μοντέλου και την ευαισθισία του σε λεπτομέριες. Συνεπώς απαιτεί εκτενή πειραματισμό ώστε τα αποτελέσματα του να καλύπτουν τις αρχικές απαιτήσεις του μοντέλου.\n",
    "- Ο κβαντισμός μπορεί να είναι στατικός ή δυναμικός. Ο στατικός εφαρμόζει κβαντοποίηση κατά την εκπαίδευση ενός μοντέλου, αυξάνει την πολυπλοκότητα του κώδικα, απαιτεί υψηλό θεωρητικό υπόβαθρο αλλά έχει εξαιρετική ακρίβεια. Ο δυναμικός εφαρμόζει κβαντοποίηση σε πραγματικό χρόνο, έχει απλή υλοποίηση και θεωρητικές απαιτήσεις αλλά μειωμένη ακρίβεια.\n",
    "\n",
    "Παρακάτω παρατίθεται ένα παράδειγμα δυναμικού κβαντισμού σε ONNX μοντέλο παρμένο από <a href=\"https://medium.com/@sulavstha007/quantizing-yolo-v8-models-34c39a2c10e2\" target=\"_blank\">εδώ. </a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88276a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_fp32 = 'preprocessed.onnx'\n",
    "model_int8 = 'dynamic_quantized.onnx'\n",
    "\n",
    "# Quantize \n",
    "quantize_dynamic(model_fp32, model_int8, weight_type=QuantType.QUInt8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd87733",
   "metadata": {},
   "source": [
    "# <u> Επιπλέον Υλικό </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ded52",
   "metadata": {},
   "source": [
    "<a href=\"https://www.youtube.com/watch?v=bx52WmQvbaE\" target=\"_blank\">\n",
    "  <img src=\"https://img.youtube.com/vi/m9fH9OWn8YM/0.jpg\" alt=\"Watch the video\" width=\"800\" height=\"450\">\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336d40b8",
   "metadata": {},
   "source": [
    "<a href=\"https://www.youtube.com/watch?v=bx52WmQvbaE\" target=\"_blank\">\n",
    "  <img src=\"https://img.youtube.com/vi/bx52WmQvbaE/0.jpg\" alt=\"Watch the video\" width=\"800\" height=\"450\">\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655ae3b",
   "metadata": {},
   "source": [
    "# <u> Ανακεφαλαίωση </u>\n",
    "\n",
    "- Περιγράφηκε λιτά το θεωρητικό υπόβαθρο της μηχανικής όρασης.\n",
    "- Παρουσιάστηκε η τεχνική διαδικασία συλλογής και επεξεργασίας των δεδομένων καθώς και η εκπαίδευσης ενός μοντέλου μηχανικής όρασης με το YOLO framework\n",
    "- Εξηγήθηκαν τρόποι αξιολόγησης ενός μοντέλου πριν εφαρμοστεί στην πράξη.\n",
    "- Δείχθηκε τρόπος εφαρμογής ενός μοντέλου αναγνώρισης σε πραγματικό χρόνο με εργαλεία όπως το opencv\n",
    "- Παρατέθηκαν τρόποι για την τροποποίηση μοντέλων για αποδοτικότερη χρήση σε συσκευές μικρής υπολογιστικής δύναμης όπως η μετατροπή ONNX και ο κβαντισμός.\n",
    "- (Ελπίζω) να ήταν σαφής η διαδικασία εκπαίδευσης ενός μοντέλου για χρήση σε μελλοντικές εφαρμογές διαδικτύου των πραγμάτων ή ρομποτικής\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
